{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ea7d911",
   "metadata": {},
   "source": [
    "# Benchmarking the DD Package\n",
    "\n",
    "MQT Core provides a benchmark suite to evaluate the performance of the DD package. This can be especially helpful if you are working on the DD package and want to know how your changes affect its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160ac9738684715a",
   "metadata": {},
   "source": [
    "## Generating results\n",
    "In order to generate benchmark data, MQT Core provides the `mqt-core-dd-eval` CMake target (which is made available by passing `-DBUILD_MQT_CORE_BENCHMARKS=ON` to CMake). This target will run the benchmark suite and generate a JSON file containing the results. To this end, the target takes a single argument which is used as a component in the resulting JSON filename."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e148f1c8",
   "metadata": {},
   "source": [
    "After running the target, you will see a `results_<your_argument>.json` file in your build directory that contains all the data collected during the benchmarking process. An exemplary `results_<your_argument>.json` file might look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee5d656",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "filepath = Path(\"../test/python/results_baseline.json\")\n",
    "\n",
    "with filepath.open(mode=\"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "    json_formatted_str = json.dumps(data, indent=2)\n",
    "    print(json_formatted_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc51fb0",
   "metadata": {},
   "source": [
    "To compare the performance of your newly proposed changes to the existing implementation, the benchmark script should be executed once based on the branch/commit you want to compare against and once in your new feature branch. Make sure to pass different arguments as different file names while running the target (e.g. `baseline` and `feature`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddecc397",
   "metadata": {},
   "source": [
    "## Running the comparison\n",
    "There are two ways to run the comparison. Either you can use the Python module `mqt.core.evaluation` or you can use the CLI. Both ways are shown below. In both cases, you need to have `mqt.core[evaluation]` installed. If you have not done so already, you can install it by running `pip install mqt.core[evaluation]` or `pip install -e .[evaluation]` in the root directory of the MQT Core repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddecc399",
   "metadata": {},
   "source": [
    "### Using the Python package\n",
    "The Python package provides a function `compare` that can be used to compare two generate result files. The function takes two arguments, the file path of the baseline json and the file path of the json results from your changes. The function will then print a detailed comparison. An exemplary run is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e5883e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mqt.core.evaluation import compare\n",
    "\n",
    "baseline_path = \"../test/python/results_baseline.json\"\n",
    "feature_path = \"../test/python/results_feature.json\"\n",
    "compare(baseline_path, feature_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bc0b58",
   "metadata": {},
   "source": [
    "Note that the method offers several parameters to customize the comparison. See [here](./api/mqt/core/evaluation/index.html#compare).\n",
    "An exemplary run adjusting the parameters is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5c2e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare(baseline_path, feature_path, no_split=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6032d920",
   "metadata": {},
   "source": [
    "### Using the CLI\n",
    "In an even simpler fashion, the comparison can be run from the command line via the automatically installed CLI. Examples of such runs are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4b59bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "! compare ../test/python/results_baseline.json ../test/python/results_feature.json --factor=0.2 --only_changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c0f53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! compare ../test/python/results_baseline.json ../test/python/results_feature.json --no_split --dd --task=functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466035c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "! compare ../test/python/results_baseline.json ../test/python/results_feature.json --dd --algorithm=bv --num_qubits=1024"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
