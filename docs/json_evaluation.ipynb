{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ea7d911",
   "metadata": {},
   "source": [
    "# MQT Benchmarking Workflow\n",
    "\n",
    "MQT Core provides a benchmark suite to evaluate its performance. If you are developing MQT Core and want to know how the changes you made in a certain branch or commit affect the performance, this workflow is especially helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160ac9738684715a",
   "metadata": {},
   "source": [
    "## First Step\n",
    "Run the script `mqt-core/eval/eval_dd_package.cpp`. Note that convenience variables to get the current branch and commit hash are defined in the script. On the line 154, change the key `CURRENT_BRANCH` to the name that you wish to use to distinguish your results.\n",
    "A typical `results.json` file is shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "![results.json](eval/results.json.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## JSON Files\n",
    "After you run the script, by default, you will see a `results.json` file and a `results_reduced.json` file. The `results.json` file contains all the data that is collected during the benchmarking process. The `results_reduced.json` is generated from the `results.json` file whereas the fields irrelevant for the benchmark comparison are removed, and the data is aggregated that every metric will consist of a dictionary with your branch/commit name as key, and the metric as the value."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Normally, a `results_reduced.json` file should resemble the following image. We will use the reduced json file to visualize the benchmarking results."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![results_reduced.json](eval/results_reduced.json.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Running the Visualization\n",
    "In the Python module `mqt.core.evaluation`, simply calling the function `compare` while passing the file name of the reduced json will give us detailed and clear comparisons.\n",
    "Note that you should change the variables on lines 17-18 in `evaluation.py` according to the keys of benchmark data in your json file.\n",
    "Note that you can and should change the variables on lines 23-26 to customize the header of the outputs.\n",
    "An exemplary CLI output after running `compare` is shown below."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![compare-default](eval/compare_default.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that the above image only shows a comparison with default parameters. Several parameters are implemented to customize the comparison.\n",
    "- `factor`. How much does the new metric have to differ from the baseline to be considered as a significant change? Default is 0.1.\n",
    "- `only_changed`. If False, a table with the benchmarks that haven't changed significantly will be shown. Default is True.\n",
    "- `sort`. The sort order of the output tables. Default is 'ratio'. The other option is 'experiment'.\n",
    "- `no_split`. If True, the output tables (improved and worsened. Also the results that stay the same if `only_changed` in False) will be merged into one table. Default is False."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "An exemplary output with `only_changed` set False is shown below."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![compare-with-same](eval/compare_with_same.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "An exemplary output with `sort` set 'experiment' is shown below."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![compare-sort-exp](eval/compare_sort_exp.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A `no_split` example:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![compare-no-split](eval/compare_no_split.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Note\n",
    "\n",
    "If you want to compare multiple branches/commits, simply set the name in `eval_dd_package.cpp`. The `results_reduced.json` file will then have dictionaries with multiple value-key pairs with keys being the branch/commit names you set. When calling `compare`, you can set the variables in `evaluation.py` to specify which two you want to compare. The function will then automatically ignore the key(s) that are not specified and it will be analogous to comparing only two branches/commits.\n",
    "</div>"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
